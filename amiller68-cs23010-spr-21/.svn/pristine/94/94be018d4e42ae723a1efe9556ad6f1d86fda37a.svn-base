\documentclass[]{article}
%opening
\title{Writeup Document}
\author{Alex Miller}
\addtolength{\oddsidemargin}{-.875in}
\addtolength{\evensidemargin}{-.875in}
\addtolength{\textwidth}{1.75in}
\addtolength{\topmargin}{-.875in}
\addtolength{\textheight}{1.75in}
\setcounter{secnumdepth}{0}
\usepackage{cancel}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\graphicspath{ {./} }
\usepackage[ruled,vlined, linesnumbered]{algorithm2e}
\DontPrintSemicolon


\begin{document}
\maketitle

\section{Changes}
\subsection{Changes to the design}
The following addresses changes I have made to my projects design, broken down by module:
\begin{itemize}
	\item Top level modules (serial.c, parallel.c, my\_test\_serial.c, my\_test\_parallel.c)
	\begin{itemize}
		\item Initializing threads outside of timer...
		\item changing structure of the counters...threads all increment when they can, as to look more like the serial based for loop.
	\end{itemize}
	\item Algorithm implementation level (lock.c)
	\begin{itemize}
		\item 
	\end{itemize}
	\item Testing script (test\_script.sh)
	\begin{itemize}
		\item 
	\end{itemize}
\end{itemize}

\subsection{Changes to Test plan:}


\section{Results}
The performance data for experiments 1 through 5 are stored in a folder called $hw2/exp\_data$. This folder contains the .csv files that describe performance data for a given experiment. This data was collected by running implementations for given number of trials, calculating measurements per trial, and taking median values. As such there is no other collected data other than median speedup, worker rates, and dispatcher rates. That data was deleted as it did not serve the purpose of analysis and would've have complicated my data storage scheme.
\\\\
As per how many trials I utilized; for experiments utilizing uniform and constant packets, I used 7 trials. For experiments calling for exponential packets, I used 13 trials. I did more trials than recommended to assure myself that I would not experience jumps in data; using less yielded less than ideal data. I did not want to use more than this in fear of exaggerated testing run times.
\\\\
I took this data and plotted it using Google sheets. This file can be found \href{https://docs.google.com/spreadsheets/d/1Vw1P-lnHN_f6wwxxaazGUFCf4o0oCOopKKm607_WKG0/edit?usp=sharing}{here}. A copy of this data is also available in a .pdf that can be located at $HW2/hw2\_result\_data.pdf$. Please refer to one of these to verify my data.

\section{Analysis}

\subsection{Experiment 1}
As the data demonstrates, serial\_queue.c experienced some slow-down in relation to serial.c. However, this effect seemed to mitigated by larger values of $W$. In some cases serial\_queue.c experienced speedup, yet these measurements are probably due to aberrations in the testing environment; serial\_queue.c is functionally equivalent to serial.c, but with increased overhead, such speedup cannot be explained by performance.
\\
Worker rates (N * T / serial\_queue runtime) tended to not be affected by the value of $N$. As I suggested in my hypotheses, they do seem to be correlated with $W$, but in the opposite sense. I expected "that worker rate go up for larger values of $W$." However, the data clearly shows the opposite. This is probably explained by two factors. For one, higher values of $W$ mean that each packet takes longer to process, effectively lowering the rate at which a worker can process packets. For two, within this experiment, higher values of $W$ are balanced out with inversely proportional values of $T$; less packets, and more work per packet, means that the worker thread gets through packets slower.
\\
However, I was mostly right in my contention that slowdown would "be dwarfed or otherwise obscured by increasingly large values of $n$, $T$, and $W$." The data shows that trials utilizing larger values of $N$ experienced less slowdown; the same may be said to be true of the trials for $N = 9, 14$. However, the data doesn't necessarily show increases in speedup with increases in $W$. Moreover, this relation is complicated by the fact that $T$ decreases as $N$ and $W$ increase in these trials.
\\
Additionally, if my base contention was that we should see decreases in slowdown due to overhead for larger input spaces, the fact that all trials in Experiment 1 used the same input space should support a more constant level of slowdown in these trials. Therefore there is a compelling sense in which the relative speedups for varying values of $N$ might in themselves be aberrations. 
\\
Therefore I was perhaps supported in my estimation that slowdown is obscured by a function of input space, but wrong to think that worker rates would go up with $W$. 

\subsection{Experiment 2}
The data demonstrates the dispatcher's ability to distribute packets suffers as the number of worker threads grows. This confirms my earlier hypothesis that "the ratio of $(n - 1) * T : runtime$ [should] shrink for larger values of $n$," however my reasoning might need to be updated.
\\
I stated that I thought this effect would be explained by increased overhead from the maintenance of more threads. However, I think there might be an alternative reason. As all the packets generated in this experiment are trivially small, worker threads can probably read packets faster than the dispatcher can fill their queues. More worker threads means that the dispatcher must spend time waiting on and filling more queues, meaning many worker threads waste time waiting on empty queues. This explains why we would see the dispatcher rate decrease for larger values of $N$.
\subsection{Experiment 3}
The data demonstrates that larger values of $W$ are associated with larger measured speedup. The data also shows that speedup increases with $N$, but experiences diminishing returns, and even drops in performance for larger values of $N$ ($N = 28$).
\\
Overall the data supports my earlier hypothesis that "In comparing the observed speedup for different values of $W$, we should expect increased speedup for larger values of $W$. However, given a value of $W$, we should expect to possibly see some speedup for some values of $n$, if only accompanied by diminishing returns on speedup for larger values of $n$. The curves we might expect from these plots  would be concave downward in shape."
\\
My reasoning was that higher values of $N$ and $W$ represented larger input spaces, which offer greater opportunity for parallelization. This was hedged with the further assertion that larger values of $N$ would also entail increased slowdown due to increased overhead. However, another explanation could be that, by attempting to use 28 threads, which is more cores than the system supports, we added overhead without performance, explaining why we could see drops in performance when moving from $N = 14$ to $N = 28$. This entails that we should see the same effects in other experiments, violating our expectations of linear speedup.
\subsection{Experiment 4}
The data demonstrates a similar downward curve as cited in Experiment 3. Likewise, we see the same increases in speedup for lager values of $W$ as well as decreasing returns for larger values of $N$. Interestingly enough we don't see such a drop off in the graph for $W = 1000$, but that could be an aberration.
\\
We do not see increased levels of speedup by using uniform packet distributions over the constant generator used in Experiment 3. Therefore the data does not confirm my hypothesis that greater amounts of speedup would be possible using a uniform packet generator due to increased opportunity for parallelization. We do not see near-linear speedup as I predicted
\\ 
As before, part of our observations are explained by the difficulties entailed in using 28 threads. As to why we don't see near linear speedup; it is perhaps because I misunderstood how the uniform generator works; the input space is not de-facto larger, it is just distributed uni-formally rather than constantly.


\subsection{Experiment 5}
The data demonstrates the same downward curves as cited in Experiments 3 and 4. Moreover, these curves demonstrate a loss in performance in relation to the prior two experiments. Likewise, we see the same increases in speedup for lager values of $W$ as well as decreasing returns for larger values of $N$. Interestingly enough we don't see such a drop off in the graph for $W = 1000$, but that could be an aberration.
\\
We do not see increased levels of speedup by using exponential packet distributions over the constant generator used in Experiment 3. Therefore the data does not confirm my hypothesis that greater amounts of speedup would be possible using a exponential packet generator due to increased opportunity for parallelization. We do not see linear speedup as I predicted.
\\ 
As before, part of our observations are explained by the difficulties entailed in using 28 threads. As to why we don't see near linear speedup; it is perhaps because I misunderstood how the exponential generator works; the input space is not de-facto larger, it is just distributed exponentially rather than constantly. This also explains why we should see relative losses in performance over Experiments 3 and 4; because the packets are distributed across sources with random exponential functions, some workers end up with really large packets to work on while other have already finished. This detracts from the benefits of parallelization and explains why we should see less speedup while using an exponential packet generator over a constant over uniform one.
\section{Theory Questions}
Book problems 25, 29, 30, 31, 32

\subsection{Problem 25}
Dropping condition L2 would be equivalent to violating the requirements of program order; $H$ would no longer have to abide by a legal sequential ordering.

\subsection{Problem 29}
No. The property leaves no guarantee that a thread can access $x$ in a finite amount of steps. Given an infinite amount of time to return, it is not clear why an infinite number of method calls could not each wait an infinite amount of time. Therefore the following property does not describe a wait free object.

\subsection{Problem 30}
Since all method calls completed in an infinite amount of time, the object must be lock-free. There is no last method call that is left waiting.

\subsection{Problem 31}
Consider an infinite history $H$ in which $m$ is called an infinite amount of times, $i$. On the $i + 1$th call, $m$ will return after $2^{\infty + 1}$ steps. Since this call neither returns in a finite number of steps nor has a finite bound on the number of steps it takes to return, it is neither wait-free nor bounded wait-free.


\subsection{Problem 32}
HWQueue is supposed to designate a queue; therefore it should support a FIFO ordering.
\\\\
Line 15 is not an appropriate linearization point for enq() for the reason that it does not enforce proper ordering. For example, let thread-A call line 15 prior to thread-B. If line 15 was a true linearization point for enq(), then this would be reflected in the state of the queue; however this is not the case.
\\
WLOG, let thread-A(enq(x)) $\rightarrow$ thread-B(enq(y)); more specifically thread-A calls line 15 prior to thread-B. However, say that thread-B calls line 16 prior to thread-A. Then, there is a possibility that some other thread C calls deq() after thread-B has set an item in the queue, but before thread-A has had the chance. Therefore, even though thread-A(enq(x)) $\rightarrow$ thread-B(enq(y)), thread-C(deq() = y) $\rightarrow$ thread-C(deq() = x) is a possible history as well. This violates our FIFO requirement.
\\
Therefore, line 15 is not a linearization point because it alone does not enforce a legal queue structure; it does not make the effect of the method call visible to other method calls.
\\\\
Line 16 is also not a linearization point, for much the same reason. Consider the example detailed above. If line 16 were truly a linearization point, then thread-C(deq() = y) $\rightarrow$ thread-C(deq() = x) would need to be true in all cases. However, consider the possibility that thread-C dequeues after thread-A has called line 16, in which case thread-C(deq() = x) $\rightarrow$ thread-C(deq() = y). In such a case, line 16 is not a linearization point. Though thread-B called it first, thread-A's object got dequeued first. Therefore line 16 is not a linearization point because it does not make the effect of the method call visible to other method calls.


	
	
\end{document}
